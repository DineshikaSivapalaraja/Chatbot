# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1958Q9dMkVUrSeFJtw4FxC5S7spV_qy9o
"""

pip install transformers

from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration

# Load the model and tokenizer
model_name = "facebook/blenderbot-400M-distill"
tokenizer = BlenderbotTokenizer.from_pretrained(model_name)
model = BlenderbotForConditionalGeneration.from_pretrained(model_name)

def chatbot_response(user_input):
    inputs = tokenizer(user_input, return_tensors="pt")  # Tokenize input
    reply_ids = model.generate(**inputs)  # Generate response
    response = tokenizer.decode(reply_ids[0], skip_special_tokens=True)  # Decode response
    return response

print("Chatbot: Hi! How can I help you? (Type 'exit' to stop)")

while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        print("Chatbot: Goodbye!")
        break

    response = chatbot_response(user_input)
    print(f"Chatbot: {response}")